# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (после прогонки укажите строки/столбцы)
- Признаки: числовые
- Пропуски: проверить по выводу в ноутбуке
- "Подлости": разные шкалы, шумовые признаки → нужно масштабирование

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (указать)
- Признаки: числовые
- Пропуски: проверить
- "Подлости": нелинейные формы + выбросы → KMeans часто хуже DBSCAN

### 1.3 Dataset C

- Файл: `S07-hw-dataset-03.csv`
- Размер: (указать)
- Признаки: числовые
- Пропуски: проверить
- "Подлости": разная плотность кластеров + шум → чувствительно к eps у DBSCAN

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- Препроцессинг: SimpleImputer(median) + StandardScaler для всех числовых; одинаково для всех моделей каждого датасета.
- Поиск гиперпараметров:
  - KMeans: k ∈ [2..10], `random_state=42`, `n_init=10`; выбор по silhouette (доп. CH).
  - DBSCAN: `eps ∈ {0.3, 0.5, 0.8}`, `min_samples ∈ {5,10}`; выбор по silhouette на ненoise-точках.
- Метрики: silhouette / Davies-Bouldin / Calinski-Harabasz; для DBSCAN метрики считаются по точкам без шума, фиксируется доля шума.
- Визуализация: PCA(2D) для лучшего решения каждого датасета; silhouette vs k для KMeans. (t-SNE опущен.)

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

Минимум (для каждого датасета):

- KMeans (поиск `k`, фиксировали `random_state`, `n_init`)
- DBSCAN (`eps`, `min_samples`, доля шума)

Опционально: третий метод / дополнительные варианты параметров.

## 4. Results

Для каждого датасета – краткая сводка результатов (подставьте после прогонки).

### 4.1 Dataset A

- Лучший метод и параметры: (смотрите `best_configs.json`)
- Метрики (silhouette / DB / CH): (из `metrics_summary.json`)
- Если был DBSCAN: доля шума и комментарий
- Коротко: почему это решение выглядит разумным именно для этого датасета

### 4.2 Dataset B

- Лучший метод и параметры: (...)
- Метрики: (...)
- Доля шума: (...)
- Комментарий: (...)

### 4.3 Dataset C

- Лучший метод и параметры: (...)
- Метрики: (...)
- Доля шума: (...)
- Комментарий: (...)

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- Где KMeans "ломается" и почему?
- Где DBSCAN выигрывает и почему?
- Что сильнее всего влияло на результат (масштабирование, выбросы, плотность, пропуски)?

### 5.2 Устойчивость (обязательно для одного датасета)

- Проверка: 5 запусков KMeans на dataset-01 с разными seed; метрика ARI между разбиениями.
- Результат: (подставьте значения ARI из ноутбука).
- Вывод: устойчиво/неустойчиво и почему.

### 5.3 Интерпретация кластеров

- Как интерпретировали кластеры: профили признаков (средние/медианы) или другие наблюдения.
- 3-6 строк выводов.

## 6. Conclusion

4-8 коротких тезисов: чему научились про кластеризацию, метрики и корректный протокол unsupervised-эксперимента.


