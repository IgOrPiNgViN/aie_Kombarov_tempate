# S03 – eda_cli: мини-EDA для CSV

Небольшое CLI-приложение для базового анализа CSV-файлов.
Используется в рамках Семинара 03 курса «Инженерия ИИ».

## Требования

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) установлен в систему

## Инициализация проекта

В корне проекта:

```bash
uv sync
```

Эта команда:

- создаст виртуальное окружение `.venv`;
- установит зависимости из `pyproject.toml`;
- установит сам проект `eda-cli` в окружение.

## Запуск CLI

### Команда `overview` – краткий обзор

```bash
uv run eda-cli overview data/example.csv
```

Параметры:

- `--sep` – разделитель (по умолчанию `,`);
- `--encoding` – кодировка (по умолчанию `utf-8`).

### Команда `report` – полный EDA-отчёт

```bash
uv run eda-cli report data/example.csv --out-dir reports
```

#### Базовые параметры

| Параметр | По умолчанию | Описание |
|----------|--------------|----------|
| `--out-dir` | `reports` | Каталог для отчёта |
| `--sep` | `,` | Разделитель в CSV |
| `--encoding` | `utf-8` | Кодировка файла |

#### Новые параметры (HW03)

| Параметр | По умолчанию | Описание |
|----------|--------------|----------|
| `--max-hist-columns` | `6` | Максимум числовых колонок для гистограмм |
| `--top-k-categories` | `5` | Сколько top-значений выводить для категориальных признаков |
| `--title` | `EDA-отчёт` | Заголовок отчёта в `report.md` |
| `--min-missing-share` | `0.1` | Порог доли пропусков для выделения проблемных колонок |
| `--high-cardinality-threshold` | `50` | Порог уникальных значений для определения высокой кардинальности категориальных признаков |
| `--zero-share-threshold` | `0.5` | Порог доли нулей в числовых колонках для эвристики «много нулей» |

#### Пример вызова с новыми параметрами

```bash
uv run eda-cli report data/example.csv \
    --out-dir reports_example \
    --title "Анализ пользовательских данных" \
    --top-k-categories 10 \
    --min-missing-share 0.05 \
    --max-hist-columns 8 \
    --high-cardinality-threshold 100 \
    --zero-share-threshold 0.3
```

### Выходные артефакты

В результате в каталоге `reports/` появятся:

- `report.md` – основной отчёт в Markdown;
- `summary.csv` – таблица по колонкам;
- `missing.csv` – пропуски по колонкам;
- `correlation.csv` – корреляционная матрица (если есть числовые признаки);
- `top_categories/*.csv` – top-k категорий по строковым признакам;
- `hist_*.png` – гистограммы числовых колонок;
- `missing_matrix.png` – визуализация пропусков;
- `correlation_heatmap.png` – тепловая карта корреляций.

## Эвристики качества данных

В `core.py` реализованы следующие эвристики:

### Базовые эвристики

- `too_few_rows` – датасет содержит менее 100 строк;
- `too_many_columns` – датасет содержит более 100 колонок;
- `too_many_missing` – максимальная доля пропусков превышает 50%.

### Новые эвристики (HW03)

- `has_constant_columns` – есть колонки, где все значения одинаковые;
- `has_high_cardinality_categoricals` – есть категориальные признаки с очень большим числом уникальных значений (настраивается через `--high-cardinality-threshold`, по умолчанию: 50);
- `has_suspicious_id_duplicates` – обнаружены дубликаты в колонках с «id» в названии;
- `has_many_zero_values` – в числовых колонках большая доля нулей (настраивается через `--zero-share-threshold`, по умолчанию: 50%).

Все эвристики учитываются в расчёте интегрального показателя `quality_score`.

## Тесты

```bash
uv run pytest -q
```

Тесты покрывают:
- базовые функции суммаризации датасета;
- расчёт пропусков и корреляций;
- все новые эвристики качества данных.
